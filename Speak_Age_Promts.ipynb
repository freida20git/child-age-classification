{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyCrdcHDsI1G"
      },
      "source": [
        "# SpeakAge - Predicting Child Age from Conversation Data\n",
        "Author: Adi Salmon, Freida Barnatan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj7hEDE9sGn4",
        "outputId": "07bc7840-2739-4386-f2dd-b8ae9554d40d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMnZWT87MTd2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Final_Project/dataset_with_child_features.csv'\n",
        "df_loaded = pd.read_csv(file_path)\n",
        "\n",
        "df_loaded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC5lXuAtBnpU"
      },
      "source": [
        "## Evaluate Model Using Prompts\n",
        "\n",
        "In this section, we evaluate the child age prediction model by providing GPT with prompts containing few-shot examples and conversation context. We compare the model’s predicted age groups with the true age_bin_readable values, compute accuracy, and analyze mispredictions to understand the impact of prompt design on performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvMZZPhU6OGK"
      },
      "source": [
        "### Predict Child Age Groups Using GPT-3.5\n",
        "\n",
        "This section improves the previous approach by including 1–2 adult lines before each child utterance as context. This gives GPT more information about what the child is responding to, improving prediction accuracy. The pipeline handles token limits, cleans special tokens, truncates long conversations, and computes accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XXPFRdw5wC8",
        "outputId": "16bc7f05-fb6b-4def-cb23-abf776c2a87f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on random sample: 65.00%\n",
            "                                           conversation age_bin_clean  \\\n",
            "1416  [(CHI, this is for you Mama), (CHI, this is fo...           3-5   \n",
            "1680  [(ADU, what do you wanna play with?), (CHI, we...           3-5   \n",
            "810   [(MOT, come here Chi come over here), (CHI, yo...           5-6   \n",
            "476   [(MOT, let me try it on), (MOT, I might like i...           3-5   \n",
            "1829  [(UNK, what that lady's name?), (UNK, Steve do...           3-5   \n",
            "623   [(INV, I want you to give us a look at this wi...           3-5   \n",
            "462   [(GMA, UNRECOGNIZED_WORD turn the table over a...           3-5   \n",
            "\n",
            "     predicted_age_bin_clean  \n",
            "1416                     2-3  \n",
            "1680                     2-3  \n",
            "810                      2-3  \n",
            "476                      2-3  \n",
            "1829                     2-3  \n",
            "623                      2-3  \n",
            "462                      5-6  \n"
          ]
        }
      ],
      "source": [
        "# ------------------ HEADER ------------------\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import ast\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=\"-- \")\n",
        "\n",
        "# ------------------ DATA PREPARATION ------------------\n",
        "\n",
        "# Ensure 'conversation' column is proper Python list\n",
        "df['conversation'] = df['conversation'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Clean special tokens\n",
        "def clean_text(text):\n",
        "    return text.replace(\"UNRECOGNIZED_WORD\", \"[unintelligible]\").replace(\"UNK\", \"[unknown]\")\n",
        "\n",
        "# ------------------ FUNCTION: CHILD LINES + CONTEXT ------------------\n",
        "\n",
        "def conv_to_text_with_context(conv_list, context_window=1):\n",
        "    \"\"\"\n",
        "    Extract child lines with a few lines of adult context.\n",
        "    context_window: number of previous lines to include before CHI line.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for i, item in enumerate(conv_list):\n",
        "        if isinstance(item, (tuple, list)) and len(item) == 2:\n",
        "            speaker, text = item\n",
        "            if speaker == 'CHI':\n",
        "                # Include previous 'context_window' adult lines\n",
        "                start_idx = max(0, i - context_window)\n",
        "                context_lines = []\n",
        "                for j in range(start_idx, i):\n",
        "                    prev_speaker, prev_text = conv_list[j]\n",
        "                    if prev_speaker != 'CHI':\n",
        "                        context_lines.append(f\"{prev_speaker}: {clean_text(prev_text)}\")\n",
        "                # Add current CHI line\n",
        "                context_lines.append(f\"{speaker}: {clean_text(text)}\")\n",
        "                lines.append(\" \".join(context_lines))\n",
        "    return \" \".join(lines)\n",
        "\n",
        "# ------------------ STEP 1: Build few-shot examples ------------------\n",
        "\n",
        "age_bins = df['age_bin_readable'].unique()\n",
        "n_examples_per_bin = 2  # examples per age bin\n",
        "few_shot_examples = \"\"\n",
        "\n",
        "for bin_val in age_bins:\n",
        "    few_rows = df[df['age_bin_readable'] == bin_val].sample(\n",
        "        n=min(n_examples_per_bin, len(df[df['age_bin_readable'] == bin_val])), random_state=42\n",
        "    )\n",
        "    for _, row in few_rows.iterrows():\n",
        "        conv_text = conv_to_text_with_context(row['conversation'], context_window=1)\n",
        "        # Truncate to 200 words to avoid token overflow\n",
        "        conv_words = conv_text.split()\n",
        "        if len(conv_words) > 200:\n",
        "            conv_text = \" \".join(conv_words[:200])\n",
        "        few_shot_examples += f\"\"\"\n",
        "Conversation: {conv_text}\n",
        "Age group: {row['age_bin_readable']}\n",
        "\"\"\"\n",
        "\n",
        "# ------------------ STEP 2: Sample test rows ------------------\n",
        "\n",
        "n_samples = min(20, len(df))\n",
        "sample_df = df.sample(n=n_samples, random_state=24).copy()\n",
        "\n",
        "# ------------------ STEP 3: Build prompt template ------------------\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "You are a child development expert.\n",
        "\n",
        "Below are examples of conversations with their correct age groups:\n",
        "{few_shot_examples}\n",
        "\n",
        "Now, given the following conversation between a mother (MOT) and her child (CHI), estimate the child's age and assign it to an age group.\n",
        "\n",
        "Important:\n",
        "- \"CHI\" means child\n",
        "- \"MOT\" means mother\n",
        "- \"INV\" means interviewer\n",
        "- \"ADU\", \"GMA\", \"UNK\" are other adults\n",
        "- Focus on the child's words (vocabulary, sentence length, complexity) to decide the age group\n",
        "\n",
        "Conversation: {{conversation_text}}\n",
        "\n",
        "Answer format: Age group: <{', '.join(age_bins)}>\n",
        "\"\"\"\n",
        "\n",
        "# ------------------ STEP 4: Generate predictions ------------------\n",
        "\n",
        "predicted_age_bins = []\n",
        "\n",
        "for conv in sample_df['conversation']:\n",
        "    conv_text = conv_to_text_with_context(conv, context_window=1)\n",
        "    # Truncate long text to 300 words\n",
        "    conv_words = conv_text.split()\n",
        "    if len(conv_words) > 300:\n",
        "        conv_text = \" \".join(conv_words[:300])\n",
        "\n",
        "    prompt = prompt_template.format(conversation_text=conv_text)\n",
        "\n",
        "    # Call GPT\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    pred = response.choices[0].message.content.strip()\n",
        "    pred_clean = pred.split(\":\")[-1].strip()\n",
        "    predicted_age_bins.append(pred_clean)\n",
        "\n",
        "# ------------------ STEP 5: Store predictions and evaluate ------------------\n",
        "\n",
        "sample_df['predicted_age_bin_clean'] = predicted_age_bins\n",
        "sample_df['age_bin_clean'] = sample_df['age_bin_readable'].str.extract(r'(\\d-\\d)')[0]\n",
        "sample_df['correct'] = sample_df['predicted_age_bin_clean'] == sample_df['age_bin_clean']\n",
        "\n",
        "# Accuracy\n",
        "accuracy = sample_df['correct'].mean()\n",
        "print(f\"Accuracy on random sample: {accuracy:.2%}\")\n",
        "\n",
        "# Show mispredictions\n",
        "misclassified = sample_df[sample_df['correct'] == False][['conversation','age_bin_clean','predicted_age_bin_clean']]\n",
        "print(misclassified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AkOxYHeK4Ng"
      },
      "source": [
        "### Improve predict Child Age Groups Using GPT-3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03If4ppi5O3M",
        "outputId": "722a0984-494c-418a-f8b2-3cb788e289b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot examples prepared for all age bins.\n",
            "Accuracy on random sample: 70.00%\n",
            "                                           conversation age_bin_clean  \\\n",
            "1416  [(CHI, this is for you Mama), (CHI, this is fo...           3-5   \n",
            "810   [(MOT, come here Chi come over here), (CHI, yo...           5-6   \n",
            "623   [(INV, I want you to give us a look at this wi...           3-5   \n",
            "462   [(GMA, UNRECOGNIZED_WORD turn the table over a...           3-5   \n",
            "267   [(INV, will you help me make a nice picture?),...           2-3   \n",
            "1590  [(MOT, what do you think?), (CHI, there's two ...           2-3   \n",
            "\n",
            "     predicted_age_bin_clean  \n",
            "1416                     2-3  \n",
            "810                      3-5  \n",
            "623                      2-3  \n",
            "462                      5-6  \n",
            "267                      3-5  \n",
            "1590                     3-5  \n"
          ]
        }
      ],
      "source": [
        "# ------------------ HEADER ------------------\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import ast\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=\"---\")\n",
        "\n",
        "# ------------------ DATA PREPARATION ------------------\n",
        "\n",
        "# Ensure 'conversation' column is a proper Python list\n",
        "df['conversation'] = df['conversation'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Function to convert conversation tuples to text\n",
        "def conv_to_text(conv_list, keep_speaker=None):\n",
        "    \"\"\"\n",
        "    Convert conversation tuples to text.\n",
        "    keep_speaker: 'CHI' to keep only child lines, None to keep all speakers.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for item in conv_list:\n",
        "        if isinstance(item, (tuple, list)) and len(item) == 2:\n",
        "            speaker, text = item\n",
        "            if keep_speaker is None or speaker == keep_speaker:\n",
        "                lines.append(f\"{speaker}: {text}\")\n",
        "    return \" \".join(lines)\n",
        "\n",
        "# Clean special tokens\n",
        "def clean_text(text):\n",
        "    return text.replace(\"UNRECOGNIZED_WORD\", \"[unintelligible]\").replace(\"UNK\", \"[unknown]\")\n",
        "\n",
        "# ------------------ STEP 1: Prepare few-shot examples ------------------\n",
        "\n",
        "age_bins = df['age_bin_readable'].unique()\n",
        "few_shot_examples = \"\"\n",
        "\n",
        "for bin_val in age_bins:\n",
        "    # Take the first example of this age bin\n",
        "    row = df[df['age_bin_readable'] == bin_val].iloc[0]\n",
        "\n",
        "    # Use only CHI lines\n",
        "    conv_text = conv_to_text([(s, clean_text(t)) for s, t in row['conversation']], keep_speaker='CHI')\n",
        "\n",
        "    # Truncate to 200 words max\n",
        "    conv_words = conv_text.split()\n",
        "    if len(conv_words) > 200:\n",
        "        conv_text = \" \".join(conv_words[:200])\n",
        "\n",
        "    few_shot_examples += f\"\"\"\n",
        "Conversation: {conv_text}\n",
        "Age group: {row['age_bin_readable']}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Few-shot examples prepared for all age bins.\")\n",
        "\n",
        "# ------------------ STEP 2: Sample test rows ------------------\n",
        "\n",
        "n_samples = min(20, len(df))\n",
        "sample_df = df.sample(n=n_samples, random_state=24).copy()\n",
        "\n",
        "# ------------------ STEP 3: Build prompt template ------------------\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "You are a child development expert.\n",
        "\n",
        "Below are examples of conversations with their correct age groups:\n",
        "{few_shot_examples}\n",
        "\n",
        "Now, given the following conversation between a mother (MOT) and her child (CHI), estimate the child's age and assign it to an age group.\n",
        "\n",
        "Important:\n",
        "- \"CHI\" means child\n",
        "- \"MOT\" means mother\n",
        "- \"INV\" means interviewer\n",
        "- \"ADU\", \"GMA\", \"UNK\" are other adults\n",
        "- Focus on the child's words (vocabulary, sentence length, complexity) to decide the age group\n",
        "\n",
        "Conversation: {{conversation_text}}\n",
        "\n",
        "Answer format: Age group: <{', '.join(age_bins)}>\n",
        "\"\"\"\n",
        "\n",
        "# ------------------ STEP 4: Generate predictions ------------------\n",
        "\n",
        "predicted_age_bins = []\n",
        "\n",
        "for conv in sample_df['conversation']:\n",
        "    # Use only CHI lines for prediction\n",
        "    conv_text = conv_to_text([(s, clean_text(t)) for s, t in conv], keep_speaker='CHI')\n",
        "    # Truncate if too long\n",
        "    conv_words = conv_text.split()\n",
        "    if len(conv_words) > 300:\n",
        "        conv_text = \" \".join(conv_words[:300])\n",
        "\n",
        "    prompt = prompt_template.format(conversation_text=conv_text)\n",
        "\n",
        "    # Call GPT\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    # Extract predicted age bin\n",
        "    pred = response.choices[0].message.content.strip()\n",
        "    pred_clean = pred.split(\":\")[-1].strip()\n",
        "    predicted_age_bins.append(pred_clean)\n",
        "\n",
        "# ------------------ STEP 5: Store predictions and evaluate ------------------\n",
        "\n",
        "sample_df['predicted_age_bin_clean'] = predicted_age_bins\n",
        "sample_df['age_bin_clean'] = sample_df['age_bin_readable'].str.extract(r'(\\d-\\d)')[0]\n",
        "sample_df['correct'] = sample_df['predicted_age_bin_clean'] == sample_df['age_bin_clean']\n",
        "\n",
        "# Accuracy\n",
        "accuracy = sample_df['correct'].mean()\n",
        "print(f\"Accuracy on random sample: {accuracy:.2%}\")\n",
        "\n",
        "# Show mispredictions\n",
        "misclassified = sample_df[sample_df['correct'] == False][['conversation','age_bin_clean','predicted_age_bin_clean']]\n",
        "print(misclassified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7V78LxHJyVn",
        "outputId": "1ac462f6-cc2b-4cd2-d1bf-f5664b43ce7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot examples prepared for all age bins.\n",
            "Accuracy on random sample: 70.00%\n",
            "                                           conversation age_bin_clean  \\\n",
            "810   [(MOT, come here Chi come over here), (CHI, yo...           5-6   \n",
            "1613  [(MOT, I came home late), (CHI, what's in here...           2-3   \n",
            "623   [(INV, I want you to give us a look at this wi...           3-5   \n",
            "970   [(MOT, Chi would you mind just telling the lad...           3-5   \n",
            "462   [(GMA, UNRECOGNIZED_WORD turn the table over a...           3-5   \n",
            "1590  [(MOT, what do you think?), (CHI, there's two ...           2-3   \n",
            "\n",
            "     predicted_age_bin_clean  \n",
            "810                      3-5  \n",
            "1613                     3-5  \n",
            "623                      2-3  \n",
            "970                      5-6  \n",
            "462                      5-6  \n",
            "1590                     3-5  \n"
          ]
        }
      ],
      "source": [
        "# ------------------ HEADER ------------------\n",
        "import pandas as pd\n",
        "import ast\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=\"---\")\n",
        "\n",
        "# ------------------ DATA PREPARATION ------------------\n",
        "# Ensure 'conversation' column is proper Python list\n",
        "df['conversation'] = df['conversation'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Clean special tokens\n",
        "def clean_text(text):\n",
        "    return text.replace(\"UNRECOGNIZED_WORD\", \"[unintelligible]\").replace(\"UNK\", \"[unknown]\").replace(\"ADU\", \"[adult]\")\n",
        "\n",
        "# Normalize child text\n",
        "def normalize_child_text(text):\n",
        "    return clean_text(text).lower()\n",
        "\n",
        "# ------------------ FUNCTION: CHILD LINES + CONTEXT ------------------\n",
        "def conv_to_text_with_context(conv_list, context_window=1):\n",
        "    \"\"\"\n",
        "    Extract child lines with a few lines of adult context.\n",
        "    context_window: number of previous lines to include before CHI line.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for i, item in enumerate(conv_list):\n",
        "        if isinstance(item, (tuple, list)) and len(item) == 2:\n",
        "            speaker, text = item\n",
        "            if speaker == 'CHI':\n",
        "                # Include previous 'context_window' adult lines\n",
        "                start_idx = max(0, i - context_window)\n",
        "                context_lines = []\n",
        "                for j in range(start_idx, i):\n",
        "                    prev_speaker, prev_text = conv_list[j]\n",
        "                    if prev_speaker != 'CHI':\n",
        "                        context_lines.append(f\"{prev_speaker}: {normalize_child_text(prev_text)}\")\n",
        "                # Add current CHI line\n",
        "                context_lines.append(f\"{speaker}: {normalize_child_text(text)}\")\n",
        "                lines.append(\" \".join(context_lines))\n",
        "    return \" \".join(lines)\n",
        "\n",
        "# ------------------ STEP 1: Build few-shot examples (1 per age bin) ------------------\n",
        "age_bins = df['age_bin_readable'].unique()\n",
        "few_shot_examples = \"\"\n",
        "\n",
        "for bin_val in age_bins:\n",
        "    # Pick 1 representative row per age bin\n",
        "    row = df[df['age_bin_readable'] == bin_val].sample(n=1, random_state=42).iloc[0]\n",
        "    conv_text = conv_to_text_with_context(row['conversation'], context_window=1)\n",
        "    conv_words = conv_text.split()\n",
        "    if len(conv_words) > 200:\n",
        "        conv_text = \" \".join(conv_words[:200])\n",
        "    few_shot_examples += f\"\"\"\n",
        "Conversation: {conv_text}\n",
        "Age group: {row['age_bin_readable']}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Few-shot examples prepared for all age bins.\")\n",
        "\n",
        "# ------------------ STEP 2: Sample test rows ------------------\n",
        "n_samples = min(20, len(df))\n",
        "sample_df = df.sample(n=n_samples, random_state=24).copy()\n",
        "\n",
        "# ------------------ STEP 3: Build prompt template ------------------\n",
        "prompt_template = f\"\"\"\n",
        "You are a child development expert.\n",
        "\n",
        "Below are examples of conversations with their correct age groups:\n",
        "{few_shot_examples}\n",
        "\n",
        "Now, given the following conversation between a mother (MOT) and her child (CHI), estimate the child's age and assign it to an age group.\n",
        "\n",
        "Important:\n",
        "- \"CHI\" means child\n",
        "- \"MOT\" means mother\n",
        "- \"INV\" means interviewer\n",
        "- \"ADU\", \"GMA\", \"UNK\" are other adults\n",
        "- Focus on the child's words, vocabulary, sentence length, and complexity to decide the age group\n",
        "\n",
        "Conversation: {{conversation_text}}\n",
        "\n",
        "Answer format: Age group: <{', '.join(age_bins)}>\n",
        "\"\"\"\n",
        "\n",
        "# ------------------ STEP 4: Generate predictions ------------------\n",
        "predicted_age_bins = []\n",
        "\n",
        "for conv in sample_df['conversation']:\n",
        "    conv_text = conv_to_text_with_context(conv, context_window=1)\n",
        "    conv_words = conv_text.split()\n",
        "    if len(conv_words) > 300:\n",
        "        conv_text = \" \".join(conv_words[:300])  # take first 300 words\n",
        "\n",
        "    prompt = prompt_template.format(conversation_text=conv_text)\n",
        "\n",
        "    # Call GPT\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    pred = response.choices[0].message.content.strip()\n",
        "    pred_clean = pred.split(\":\")[-1].strip()\n",
        "    predicted_age_bins.append(pred_clean)\n",
        "\n",
        "# ------------------ STEP 5: Store predictions and evaluate ------------------\n",
        "sample_df['predicted_age_bin_clean'] = predicted_age_bins\n",
        "sample_df['age_bin_clean'] = sample_df['age_bin_readable'].str.extract(r'(\\d-\\d)')[0]\n",
        "sample_df['correct'] = sample_df['predicted_age_bin_clean'] == sample_df['age_bin_clean']\n",
        "\n",
        "# Accuracy\n",
        "accuracy = sample_df['correct'].mean()\n",
        "print(f\"Accuracy on random sample: {accuracy:.2%}\")\n",
        "\n",
        "# Show mispredictions\n",
        "misclassified = sample_df[sample_df['correct'] == False][['conversation','age_bin_clean','predicted_age_bin_clean']]\n",
        "print(misclassified)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
